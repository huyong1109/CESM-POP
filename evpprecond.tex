\section{Error Vector Propagation (EVP) Preconditioning} \label{se:evp}
Equations (\ref{t_pcg}) and (\ref{t_psi}) show that the total execution time of the barotropic solver is the multiplication of
iteration number and the execution time per iteration.
With increasing numbers of processor cores, the execution time of computation in each iteration decreases
but the execution time of communication increases.
In order to reduce the communication cost, the preconditioning is commonly used to reduce iterations for the solver convergence
assuming the cost of preconditioning is reasonable.
Three major concerns of preconditioners for large sparse linear systems are efficiency, scalability and preprocessing costs. 
The existing ChronGear solver in POP has benefited greatly using a simple diagonal preconditioning \cite{pini1990simple, reddy2013comparison}. 
However, its convergence rate is limited and further reduction of the iteration count can significantly reduce the associated communication cost
to enhance the scalability in a large number of processors. 
The performance of both P-CSI and the existing ChronGear solvers can be further improved by a more effective preconditioner.

%The data distribution and communication cost can only tolerate lower level of factorization like ILU(0). L and U have the same nonzero structure as the lower and upper parts of A respectively, which often results in a crude approximation has a very bad effect on convergence speed. 
%More accurate ILU factorizations is not suit for ocean model, because it requires more communications and computations. 
%Also, the preprocessing cost to compute the factors is higher. 

%In 1985, Concus et al. \cite{concus1985block} used the banded approximate of the matrix inverse as a preconditioner,
%and achieved higher efficiency than other preconditioners on elliptic partial differential equations. 
%NOTE YH what is "local approximate inverse of the true operator"? 
%Smith et al. \cite{smith1992parallel} employed a polynomial preconditioning method and a local approximate-inverse method.
%POP supports a preconditioner consisting of a 9-point operator which is a local approximate-inverse of the true operator.

\subsection{Block Preconditioning}

The most widely used preconditioning techniques in sequential simulations are the incomplete factorization methods
like incomplete LU factorization (ILU) and its variants \cite{benzi2002preconditioning}.
However, they are ill-suited for parallel computers because they require a recursive operation which limits parallelization. 

Some parallelizable preconditioning methods such as polynomial preconditioning, 
approximate inverse preconditioning, multigrid preconditioning and block preconditioning 
have drawn much attention recently.
High-order polynomial preconditioning can reduce iterations as effectively as ILU, however,
its computational overhead offsets its superiority to diagonal preconditioning 
\cite{meyer1989numerical,smith1992parallel}. 
A $k$th-order polynomial preconditioner requires $k-1$ matrix vector multiplications in each iteration. 
The approximate inverse preconditioner can be highly parallelized but requires to
solve another linear system which is several times larger than the one it preconditions 
\cite{smith1992parallel,bergamaschi2007numerical}. 
This makes it even less attractive than the diagonal preconditioner.
Multigrid is another popular preconditioning method which is highly scalable and efficient in preconditioning the sparse linear systems developed from elliptic equations \cite{baker2012scaling}. 
But the cost of setting up the multigrid preconditioning is so high that it is not worth
using multigrid for iterative solvers in problems like POP, which converge in tens or hundreds of iterations. 
Block preconditioning has been commonly shown as an effective alternative in parallel computing 
\cite{concus1985block, white2011block},
which makes use of the block structure of the coefficient matrix
arising from discretization of the elliptic equations and factorizes the matrix block-by-block 
instead of point-by-point.

\begin {figure}
\centering
\includegraphics[width=0.8\linewidth]{blockpreconditioning.eps}
\caption[] {Sparsity pattern of the coefficient matrix developed from nine-point stencils. 
The whole domain is divided into $3\times3$ non-overlapping blocks.
Elements in red rectangles are coefficients between points in blocks. 
Elements in blue rectangles are coefficients between points from the $i$-th block and its neighbor blocks. \label{fig:blockprecond}}
\end{figure}

Fig \ref{fig:blockprecond} illustrates the diagram of the block precondition.
Providing that the linear system of $\mathcal{N} \times \mathcal{N}$ grids is reordered block by block with size of $n\times n$
(e.g., $\mathcal{N}/3\times \mathcal{N}/3$ in Fig. \ref{fig:blockprecond}),
the coefficient matrix $A$ can be represented by a nine-diagonal block matrix. 
Each row of this matrix contains nine sub-matrix (Fig \ref{fig:blockprecond}). %(arranged in their spatial relationship)
Each $B_i$ (red blocks) is a block matrix containing coefficients of the grids in the i-th block, which share the same structure as $A$ but has a smaller size of $n^2\times n^2$. 
$B_i^e$,$B_i^w$,$B_i^n$ and $B_i^s$ are diagonal block matrix containing coefficients of points on east, west, north and south boundaries
and the points on their respective neighboring blocks, thus having at most $3n$ nonzero elements distributed on $n$ rows. 
$B_i^{nw}$,$B_i^{ne}$,$B_i^{sw}$ and $B_i^{se}$ have only one nonzero element, representing the coefficient of corner points
and their neighboring points on the northwest, northeast, southwest and southeast blocks. 

The traditional block preconditioning method constructs the approximate inverse of $A$ by sequentially factorizing it with approximations of $B_i^{-1}$,
which is ill-suited for parallel applications. 
%the diagonal approximation or approximation from Cholesky factors of $B_i^{-1}$
On the contrary, the inverse of the block diagonal of $A$, which provides a good approximation for $A$, can be calculated naturally in parallel.
The inverse of the diagonal block matrices is  
\begin{eqnarray*}
M^{-1}=    \left [
        \begin{array}{ccccccc}
        B_1^{-1} &   &  \\
         & \ddots&  \\
        &   &  B_{m^2}^{-1} \\
    \end{array}
    \right ]
\end{eqnarray*}
Using this $M$ as a preconditioner, the preconditioning process $\textbf{x} = M^{-1}\textbf{y}$ is usually transformed into solving the sparse linear equations $B_i \textbf{x}_i = \textbf{y}_i$ for each block,
instead of explicitly constructing $B_i^{-1}$.
This parallel block preconditioning is preferable in parallel application due to less computation to solve the equations with LU decomposition
than multiplying $\textbf{y}$ by $M^{-1}$. 

\subsection{Error Vector Propagation Method}
The arithmetic complexity of solving the equations $B_i \textbf{x}_i = \textbf{y}_i$ with LU decomposition is $\mathcal{O}(n^4)$,
providing that the LU decomposition is previously initialized.
The ILU method is much more efficient only in sequential application than in parallel application
which only supports no-fill ILU that has very crude approximate solution. 
To the best of our knowledge, the most efficient alternative to solve elliptic equations is
the EVP method in terms of the iteration counts \cite{roache1995elliptic},
which marches over the whole domains based on the discretized equations with an arithmetic complexity $\mathcal{O}(n^2)$.
%EVP is an alternative of fast direct solvers for elliptic equations, 
The EVP method and its variant have been used in some ocean models (e.g., Sandia Ocean Modeling System \cite{dietrich1987ocean} and CANadian version of DIEcast \cite{sheng1998candie}).
Tseng and Chien \cite{tseng2011parallel} further employed a modified parallel EVP method based on domain-decomposition as a solver for the global ocean simulation. 

\begin {figure}[!t]
\centering
\includegraphics[width=0.8\linewidth]{evp9pmarch1.png}
\caption []{EVP marching method for nine-point stencil. The solution on point $(i+1,j+1)$ can be calculated using the equation on point $(i,j)$, providing solutions on other neighbor points of point $(i,j)$ (all magenta points).  \label {fig:evp9p}}
\end {figure}

We can discretize Equation \ref{eq:ssh} into the following form so that we can march the solution northeastward assuming all other neighboring points are exactly known (pink circles):
\begin{eqnarray}
\label{eq:evp9p}
&\eta_{i+1,j+1} = (1/A_{i,j}^{ne} )(\psi_{i,j} - A_{i,j}^0\eta_{i,j}-A_{i,j}^e\eta_{i+1,j} \nonumber\\
&-A_{i,j}^n\eta_{i,j+1}-A_{i-1,j}^{ne}\eta_{i-1,j+1} +A_{i-1,j}^e\eta_{i-1,j}\nonumber\\
&-A_{i-1,j-1}^{ne}\eta_{i-1,j-1}-A_{i,j-1}^n\eta_{i,j-1}- A_{i+1,j-1}^{ne}\eta_{i,j-1} )
\end{eqnarray}
%\textbf{I suggest you to make Fig. 5 to use a different color or label to represent the specified boundary conditions}
Fig.\ref{fig:evp9p} provides an example for a Dirichlet boundary elliptic equation $B\textbf{x} = \psi$ on a small domain. 
Let's define the interior points next to the south and west boundaries
as the initial guess points $\textbf{e}$ and those next to the north and east boundaries are the final boundary points $\textbf{f}$
(e.g., $\textbf{e}= \{E_1, \dots, E_7\}$, $\textbf{f}= \{F_1, \dots, F_7\}$ in Fig.\ref{fig:evp9p}).
If the true solution on $\textbf{e}$ is known, the exact values over the whole domain can be computed
sequentially from southwest to northeast corners, using equation \ref{eq:evp9p}. This procedure is referred as marching. 
Unfortunately, the value on $\textbf{e}$ is often not known until the elliptic equation is solved. 
However, we can get a solution $\textbf{x}$ satisfying the elliptic equation on the whole domain except on the boundary,
by first guessing the value $\textbf{x}|_\textbf{e}$ on $\textbf{e}$ and then calculating the rest using the marching method. 
Then $E=(\textbf{x} -\eta)|_\textbf{e}$ and $F=(\textbf{x} -\eta)|_\textbf{f}$ are error vectors on $\textbf{e}$ and $\textbf{f}$, respectively. 
The error vector $F$ is already known since $\textbf{f}$ are boundary points (Dirichlet boundary condition is imposed).
The relationship between the error on initial guess points and the final boundary points can be represented as $F=W*E$. 
This influence coefficient matrix $W$ can be formed by marching on the whole domain with unit vectors on the initial guess points and zero residual value
in the whole domain. 
We summarized this EVP algorithm for an elliptic equation with zero boundary in Algorithm \ref{alg:evp}. 


\begin{algorithm}[!h]
\caption{Nine-point Error Vector Propagation method}
\label{alg:evp}
\begin{algorithmic}[1]
\REQUIRE Residual $\psi$ associated with a domain containing $n\times n$ grids, $k = size(\textbf{e})=2n-5$; \\
//\qquad \textit{preprocessing }
\STATE  $\textbf{x} = \textbf{0}$
\FOR {i = 1 \TO k}
\STATE $\textbf{x}|_\textbf{e}(i) = 1$
\STATE $\textbf{x} = marching(\textbf{x},\textbf{0})$
\STATE $W(i,:) = \textbf{x}|_\textbf{f}$
\STATE $\textbf{x}|_\textbf{e}(i) = 0$
\ENDFOR 
\STATE $R = inverse(W)$ \\
//\qquad \textit{solving }
\STATE $\textbf{x}= marching(\textbf{x},\psi)$
\STATE $F = (\textbf{x} - \eta)|_\textbf{f}$
\STATE $\textbf{x}|_\textbf{e} =\textbf{x}|_\textbf{e} - R*F$
\STATE $\textbf{x} = marching(\textbf{x},\psi)$
\end{algorithmic}
\end{algorithm}

The EVP method contains two processes: preprocessing and solving. In the preprocessing step, the influence coefficient matrix
and its inverse are computed, involving a calculation of  $\mathcal{C}_{pre}= ((2n-5)* 9n^2 + (2n-5)^3 = \mathcal {O} (26n^3)$.
The solution is then directly solved in the solving step with a calculation of $\mathcal{C}_{evp}= 2* 9n^2 + (2n-5)^2 = \mathcal{O} (22n^2)$. 
The estimation of operation counts indicates that the EVP is more efficient than other direct solvers like LU decomposition
or iterative methods such as PCG for elliptic equations, if only the solving step is considered.
This makes the EVP practical in real application since the preprocessing is only needed once at the beginning
to obtain the influence coefficient matrix and its inverse.

\subsection{EVP as A Parallel Preconditioner}
The EVP method described in Algorithm \ref{alg:evp} provides an alternative to efficiently solve the elliptic equations.
%thus making it possible to use the block diagonal of $A$ as a preconditioner. 
%\textbf{not sure what the above sentence means mean EVP combined with block diagonal?}
Here, we develop a block preconditioning technique based on the EVP method in each block to further enhance
the performance of the barotropic solver in POP.
Each preconditioning solves elliptic equations $B_i \textbf{x} = \textbf{y}  (i=1,...,m^2)$ in parallel. 
%We use the original version of EVP as the local preconditioner for simplicity and efficiency. 
%We developed a new preconditioning method inspired by the block preconditioning. 

A major drawback of the EVP method is that it cannot solve any large domain problem without further modification due to
its numerical instability in marching \cite{roache1995elliptic}. 
However, the EVP can solve any small domain up to the size of $12\times 12$ with
an acceptable round-off error of $10^{-8}$ when double-precision floating-point format is used, which makes it ideal for the block preconditioning
in the parallel computing.
%\textbf{are we using single-precision? or double? I think we have to use double, right?} %should there be proof for this? 
%Even though EVP method can be adapted to solve elliptic equations on irregular geometries. 
%Thus, we construct preconditioning matrices with small blocks. 
\begin {figure}[!t]
\centering
\includegraphics[width=1.0\linewidth]{iteration.eps}
\caption[] {Average iteration number of different barotropic solvers. \label{fig:iteration}}
\end{figure}


Therefore, the drawback of the EVP becomes a great advantage in the large scale parallel computing on the other hand
(larger number of processors facilitate smaller domains).
Furthermore, we find that the coefficients related to north, south, east and west neighbors on every point are one magnitude order smaller than the others. 
Removing these coefficients reduces the cost of calculating the EVP preconditioning matrix so that the preconditioning process
can be reduced to about a half without any significant impact on the iteration number for both ChronGear and P-CSI. 
As a result, the execution time of EVP preconditioning can be expressed as $\mathcal{T'}_{p} = 14n^2\theta= 14\frac{\mathcal{N}^2}{p}\theta$. 
The actual cost of $\mathcal{T'}_{p}$ depends on the size of the local block, thus decreasing as more processor cores are used.
%In the current version of POP, using non-diagonal preconditioning methods requires an additional boundary communication after the preconditioning in each iteration. 
%However, we find that the later boundary communication can be skipped by utilizing the fact that the halo size is 2. 
Thus, the total execution time for one ChronGear and P-CSI solver step are 
\begin{eqnarray}
\label{t_evppcg}
&\mathcal{T'}_{cg}=\mathcal{K'}_{cg} (\mathcal{T'}_c + \mathcal{T'}_b+\mathcal{T'}_g )\nonumber \\
&=\mathcal{K'}_{cg} [29 \frac{\mathcal{N}^2}{p}\theta + \frac{8\mathcal{N}}{\sqrt{p}}\beta +(4+\log p)\alpha]
\end{eqnarray}
\begin{eqnarray}
\label{t_evppsi}
\mathcal{T'}_{pcsi} = \mathcal{K'}_{pcsi}(\mathcal{T'}_c + \mathcal{T'}_b ) \nonumber \\
= \mathcal{K'}_{pcsi}[26\frac{\mathcal{N}^2}{p}\theta+ 4\alpha + \frac{8\mathcal{N}}{ \sqrt{p}}\beta]
\end{eqnarray}


%\begin{table}
%\centering
%\caption{Average iteration number of different barotropic solvers. \label{tab:iteration_evp}}
%\begin{tabular}{|l|c|c|c|c|}
%%\toprule
%\hline
%Resolution & \multicolumn{2}{|c|}{1\degree} & \multicolumn{2}{|c|}{0.1\degree}\\ \hline
%Solvers& ChronGear  & P-CSI & ChronGear	 & P-CSI  \\\hline
%NONE&  540  &570	& 150	 & 160  \\\hline
%DIAG&  240  &300	& 100	 & 120 \\\hline
%EVP &  160  &200	& 50	 & 60  \\\hline
%%\bottomrule
%\end{tabular}
%\end{table}
The implementation of EVP preconditioning in POP significantly reduces the iterations in both ChronGear and P-CSI solvers. 
As shown in Fig. \ref{fig:iteration}, EVP preconditioning reduces  iterations to about one third in both 1\degree and 0.1 \degree cases, which is comparable to the approximate-inverse preconditioner proposed in \cite{smith1992parallel}. 
Although the EVP preconditioning doubles the computation in each iteration, it halves both global and boundary communication
which dominates in the barotropic execution time when large numbers of processor cores are used.
One advantage of EVP preconditioning is the low cost of solving the preconditioning matrix. 
%EVP preconditioning matrix is totally parallelized, while traditional decomposition based preconditioning requires serial processing. 
In 0.1\degree case, the cost of setting up the preconditioning matrix is less than the one of calling the solver once when 512 processor cores are used. 
This cost can be further decreased when more processors are used. 
